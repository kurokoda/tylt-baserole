apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: patroni
spec:
  serviceName: "etcd"
  replicas: 1
  template:
    metadata:
      labels:
        app: patroni
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      containers:
      - command:
        - /bin/sh
        - -ec
        - |
          HOSTNAME=$(hostname)
          SET_ID=${HOSTNAME/*-/}
          wget -qO- http://${HOSTNAME/-*/}-0.${SET_NAME}.${POD_NS}.svc.cluster.local:2379/v2/members |  sed -e $'s/]}/\\\n/g' | grep http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380 | sed -e 's/^.*"id":"\([^"]*\)".*$/\1/' >> /var/run/etcd/member_id

          collect_member() {
              while ! etcdctl member list &>/dev/null; do sleep 1; done
              etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380 | cut -d':' -f1 | cut -d'[' -f1 > /var/run/etcd/member_id
              exit 0
          }

          eps() {
              EPS=""
              for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                  EPS="${EPS}${EPS:+,}http://${HOSTNAME/-*/}-${i}.${SET_NAME}.${POD_NS}.svc.cluster.local:2379"
              done
              echo ${EPS}
          }

          member_hash() {
              etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380 | cut -d':' -f1 | cut -d'[' -f1
          }

          # re-joining after failure?
          if [ -e /var/run/etcd/default.etcd ]; then
              echo "Re-joining etcd member"
              member_id=$(cat /var/run/etcd/member_id)

              # re-join member
              ETCDCTL_ENDPOINT=$(eps) etcdctl member update ${member_id} http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380
              exec etcd --name ${HOSTNAME} \
                  --listen-peer-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380 \
                  --listen-client-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2379,http://127.0.0.1:2379 \
                  --advertise-client-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2379 \
                  --data-dir /var/run/etcd/default.etcd
          fi

          # etcd-SET_ID

          # adding a new member to existing cluster (assuming all initial pods are available)
          if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
              export ETCDCTL_ENDPOINT=$(eps)

              # member already added?
              MEMBER_HASH=$(member_hash)
              if [ -n "${MEMBER_HASH}" ]; then
                  # the member hash exists but for some reason etcd failed
                  # as the datadir has not be created, we can remove the member
                  # and retrieve new hash
                  etcdctl member remove ${MEMBER_HASH}
              fi

              echo "Adding new member"
              etcdctl member add ${HOSTNAME} http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380 | grep "^ETCD_" > /var/run/etcd/new_member_envs

              if [ $? -ne 0 ]; then
                  echo "Exiting"
                  rm -f /var/run/etcd/new_member_envs
                  exit 1
              fi

              cat /var/run/etcd/new_member_envs
              source /var/run/etcd/new_member_envs

              collect_member &

              exec etcd --name ${HOSTNAME} \
                  --listen-peer-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380 \
                  --listen-client-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2379,http://127.0.0.1:2379 \
                  --advertise-client-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2379 \
                  --data-dir /var/run/etcd/default.etcd \
                  --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380 \
                  --initial-cluster ${ETCD_INITIAL_CLUSTER} \
                  --initial-cluster-state ${ETCD_INITIAL_CLUSTER_STATE}
          fi

          for i in $(seq 0 ${SET_ID}); do
              while true; do
                  echo "Waiting for ${HOSTNAME/-*/}-${i}.${SET_NAME}.${POD_NS}.svc.cluster.local to come up"
                  ping -W 1 -c 1 ${HOSTNAME/-*/}-${i}.${SET_NAME}.${POD_NS}.svc.cluster.local > /dev/null && break
                  sleep 1s
              done
          done

          PEERS=""
          for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
              PEERS="${PEERS}${PEERS:+,}${HOSTNAME/-*/}-${i}=http://${HOSTNAME/-*/}-${i}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380"
          done

          collect_member &

          # join member
          exec etcd --name ${HOSTNAME} \
              --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380 \
              --listen-peer-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380 \
              --listen-client-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2379,http://127.0.0.1:2379 \
              --advertise-client-urls http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2379 \
              --initial-cluster-token etcd-cluster-1 \
              --initial-cluster ${PEERS} \
              --initial-cluster-state new \
              --data-dir /var/run/etcd/default.etcd
        env:
        - name: INITIAL_CLUSTER_SIZE
          value: "1"
        - name: SET_NAME
          value: etcd
        - name: POD_NS
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        image: gcr.io/google_containers/etcd-amd64:2.2.5
        imagePullPolicy: Always
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -ec
              - |
                EPS=""
                for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                    EPS="${EPS}${EPS:+,}http://${HOSTNAME/-*/}-${i}.${SET_NAME}.${POD_NS}.svc.cluster.local:2379"
                done

                HOSTNAME=$(hostname)

                member_hash() {
                    etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}.${POD_NS}.svc.cluster.local:2380 | cut -d':' -f1 | cut -d'[' -f1
                }

                echo "Removing ${HOSTNAME} from etcd cluster"

                ETCDCTL_ENDPOINT=${EPS} etcdctl member remove $(member_hash)
                if [ $? -eq 0 ]; then
                    # Remove everything otherwise the cluster will no longer scale-up
                    rm -rf /var/run/etcd/*
                fi
        name: etcd
        ports:
        - containerPort: 2380
          name: peer
          protocol: TCP
        - containerPort: 2379
          name: client
          protocol: TCP
        livenessProbe: &healthcheck
          tcpSocket:
            port: 2379
        readinessProbe:
          <<: *healthcheck
        volumeMounts:
        - mountPath: /var/run/etcd
          name: data


      #patroni
      - env:
        - name: SCOPE
          value: patroni
        - name: USE_WALE
        - name: PGROOT
          value: /home/postgres/pgdata/pgroot
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: ETCD_HOST
          value: etcd.$(POD_NAMESPACE):2379
        - name: CALLBACK_SCRIPT
          value: callback_role.py
        - name: CRONTAB
          valueFrom:
            configMapKeyRef:
              name: globalvalues
              key: PG_CRON           
        envFrom:
        - configMapRef:
            name: globalvalues
        image: registry.opensource.zalan.do/acid/spilo-9.6:1.2-p17
        imagePullPolicy: Always
        name: patroni
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -ec
              - |
                LATEST_DUMP=$(ls -Art /mnt/tylt-pg-prod/*.dump | tail -n1)
                apt-get update && apt-get install at && atd 
                echo "while ! pg_isready -d postgresql://postgres:${PGPASSWORD_ADMIN}@localhost/postgres -q; do sleep 1 ; done && pg_restore -cCOd postgres $LATEST_DUMP -U postgres --verbose" | at now
          preStop:
            exec:
              command:
              - /bin/sh
              - -ec
              - |
                /usr/bin/pg_dump -CFc tylt-production > /home/deploy/data/backups/tylt-pg-prod/tylt-production_backup_$(date -u +"%Y-%m-%dT%H-%M")_utc.dump 
        ports:
        - containerPort: 8008
          protocol: TCP
        - containerPort: 5432
          protocol: TCP
        livenessProbe: &healthcheck
          tcpSocket:
            port: 5432
        readinessProbe:
          <<: *healthcheck
        volumeMounts:
        - mountPath: /home/postgres/data
          name: storage-volume
        - mountPath: /etc/patroni
          name: patroni-config
          readOnly: true
        - name: podinfo
          mountPath: /mnt/podinfo
          readOnly: false
        - name: backup-dir
          mountPath: /mnt/tylt-pg-prod
          readOnly: PRODUCTION_VAR

      #adminer
      - image: crunchydata/crunchy-pgadmin4:centos7-9.6-1.5 
        name: pgadmin
        ports:
        - containerPort: 5050
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                echo '127.0.0.1	basic	basic' >> /etc/hosts
        env:
        - name: DEFAULT_USER
          value: 'postgres'
        - name: DEFAULT_PASSWORD
          valueFrom:
            configMapKeyRef:
              name: globalvalues
              key: PGPASSWORD_ADMIN 
        livenessProbe: &healthcheck
          tcpSocket:
            port: 5050
        readinessProbe:
          <<: *healthcheck
        volumeMounts:
        - name: backup-dir
          mountPath: /mnt/tylt-pg-prod 
          readOnly: PRODUCTION_VAR

      volumes:
      - name: data
        emptyDir: {}
      - name:  storage-volume
        emptyDir: {}
      - name: patroni-config
        secret:
          secretName: patroni
      - name: podinfo
        downwardAPI:
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "annotations"
              fieldRef:
                fieldPath: metadata.annotations
      - name: backup-dir
        hostPath:
          path: /mnt/s3-backup-share/tylt-pg-prod
